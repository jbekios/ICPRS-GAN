{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1-5X9OUkA-C2Ih1gOS9Jd7GmkTWUEpDg1)\n",
    "\n",
    "#  _Deep learning_ : GAN (Generative Adversarial Nets)\n",
    "   \n",
    "<center>\n",
    "    <img src='images/ladron-gan.png'style=\"width: 600px;\">\n",
    "    <sub><sup>https://dzone.com/articles/working-principles-of-generative-adversarial-netwo</sup></sub> \n",
    "</center>\n",
    "\n",
    "**Profesor**: Dr. Juan Bekios Calfa - **ICPRS 2021**\n",
    "\n",
    "<sub><sup>Tutorial: GANS. Sensio Artificial Intelligence [link](https://sensioai.com/blog/051_gans)</sup></sub> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n",
    "GANs (_generative adversarial networks_) es una arquitectura de red neuronal propuesta en 2014 por Ian Goodfellow y otros con el objetivo de obtener modelos capaces de generar datos realistas, principalmente imágenes. \n",
    "\n",
    "Pese a la simplicidad de la idea original, se tardó varios años en superar varias de las dificultades que presenta su entrenamiento. Hoy en día, sin embargo, se utilizan para obtener resultados espectaculares. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arquitectura\n",
    "\n",
    "La arquitectura básica de las GANs está formada por dos redes neuronales:\n",
    "\n",
    "* **El generador**: recibe a la entrada valores aleatorios (normalmente obtenidos de una distribución de tipo gausiana) y da a la salida una imagen. Puedes ver la entrada aleatoria como una representación latente (o codificación) de la imagen generada.\n",
    "* **El discriminador**: recibe a la entrada una imagen (real o generada por el generador) y tiene que decidir si bien la imagen es real o falsa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arquitectura\n",
    "\n",
    "<center>\n",
    "    <img src='images/gan-arq-animada.gif'style=\"width: 1000px;\">\n",
    "</center>\n",
    "\n",
    "<sub><sup>https://anderfernandez.com/blog/como-crear-una-red-generativa-antagonica-gan-en-python/</sup></sub> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Para poder entrenar una GAN:\n",
    "\n",
    "* En una **primera fase**, entrenamos el discriminador. En cada iteración, le daremos un batch compuesto de imágenes reales obtenidas del dataset e imágenes falsas generadas por el generador. Definiremos un conjunto de etiquetas a 0 para las imágenes falsas y 1 para las reales. Entonces, optimizaremos el discriminador (utilizando una función de pérdida de tipo binary cross-entropy mejorando así sus capacidades de distinguir imágenes falsas de reales.\n",
    "* En la **segunda fase**, entrenamos el generador. En cada iteración, le daremos un batch compuesto de ruido aleatorio para que genere imágenes. Estas imágenes son introducidas en el discriminador, cuyas salidas (etiquetas real/falso) son comparadas con un conjunto de etiquetas definidas como reales. Entonces, optimizaremos el generador (utilizando de nuevo la misma función de pérdida) de manera que el generador actualizará sus pesos para generar imágenes que engañen al discriminador.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GAN simple\n",
    "\n",
    "Vamos a ver cómo podemos implementar esta arquitectura y proceso de entrenamiento con la implementación de una GAN muy simple para generar imágenes del _dataset Fashion MNIST_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Root directory for the dataset\n",
    "data_root = 'data/celeba'\n",
    "# Spatial size of training images, images are resized to this size.\n",
    "image_size = 64\n",
    "\n",
    "celeba_data = datasets.CelebA(data_root,\n",
    "                              download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.Resize(image_size),\n",
    "                                  transforms.CenterCrop(image_size),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                       std=[0.5, 0.5, 0.5])\n",
    "                              ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, trainset):\n",
    "    self.imgs = torch.tensor([np.array(i[0]).flatten() / 255. for i in trainset], dtype=torch.float, device=device)\n",
    "    self.labels = torch.tensor([i[1] for i in trainset], dtype=torch.long, device=device)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.imgs)\n",
    "\n",
    "  def __getitem__(self, ix):\n",
    "    return self.imgs[ix], self.labels[ix]\n",
    "\n",
    "train = Dataset(trainset)\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "img, label = train[0]\n",
    "img.shape, img.dtype, img.max(), img.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\n",
    "\n",
    "imgs, labels = next(iter(dataloader))\n",
    "imgs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "La base de datos está formado por **60000 imágenes** de baja resolución (**28 x 28 píxeles**, en blanco y negro) y contiene 10 tipos prendas de ropa (camisetas, pantalones, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "r, c = 3, 5\n",
    "plt.figure(figsize=(c*3, r*3))\n",
    "for row in range(r):\n",
    "    for col in range(c):\n",
    "        index = c*row + col\n",
    "        plt.subplot(r, c, index + 1)\n",
    "        ix = random.randint(0, len(train)-1)\n",
    "        img, label = train[ix]\n",
    "        plt.imshow(img.reshape(28,28).cpu(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(classes[label.item()])\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementación GAN simple\n",
    "\n",
    "El objetivo es entrenar una GAN que sea capaz de generar imágenes similares a las que tenemos en la base de datos a partir de valores aleatorios (ruido). \n",
    "\n",
    "Se implementará **un generador** y **un discriminador**. Para esta implementación simple usaremos la misma arquitectura para ambas redes, un **Perceptrón Multicapa (MLP)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def block(n_in, n_out):\n",
    "  return nn.Sequential(\n",
    "      nn.Linear(n_in, n_out),\n",
    "      nn.ReLU(inplace=True)\n",
    "  )\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super().__init__()\n",
    "    self.input_size = input_size\n",
    "    self.fc1 = block(input_size, 150)\n",
    "    self.fc2 = block(150, 100)\n",
    "    self.fc3 = nn.Linear(100, output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc1(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.fc3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arquitectura\n",
    "\n",
    "La **red neuronal multicapa** está formado por 3 capas lineales (cada capa seguida de una activación relu). En función del número de entradas y salidas definiremos las diferentes redes. Por ejemplo, el generador recibirá un vector con 30 valores aleatorios y nos dará a la salida un vector de 28 x 28 valores (igual que las imágenes del dataset).\n",
    "\n",
    "### Generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_in, n_out = 30, 28*28\n",
    "generator = MLP(n_in, n_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generador inicializado\n",
    "\n",
    "Como los pesos son aleatorios y no han sido entrenados. Si probamos con una **entrada aleatoria** la **salida esperada** será una **imagen con ruido**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "output = generator(torch.randn(64, 30))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(output[40].reshape(28,28).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discriminador\n",
    "\n",
    "Recibirá a la entrada una imagen (28 x 28 valores) y a la salida nos dará una clasificación binaria (real o falso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = MLP(28*28, 1)\n",
    "output = discriminator(torch.randn(64, 28*28))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entrenando ambas redes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "def fit(g, d, dataloader, epochs=30, crit=None):\n",
    "  g.to(device)\n",
    "  d.to(device)\n",
    "  g_optimizer = torch.optim.Adam(g.parameters(), lr=3e-4)\n",
    "  d_optimizer = torch.optim.Adam(d.parameters(), lr=3e-4)\n",
    "  crit = nn.BCEWithLogitsLoss() if crit == None else crit\n",
    "  g_loss, d_loss = [], []\n",
    "  mb = master_bar(range(1, epochs+1))\n",
    "  hist = {'g_loss': [], 'd_loss': []}\n",
    "  for epoch in mb:\n",
    "    for X, y in progress_bar(dataloader, parent=mb):\n",
    "      #X, y = X.to(device), y.to(device)  \n",
    "      # entrenamos el discriminador\n",
    "      g.eval()\n",
    "      d.train()\n",
    "      #   generamos un batch de imágenes falsas\n",
    "      noise = torch.randn((X.size(0), g.input_size)).to(device)\n",
    "      genenerated_images = g(noise)\n",
    "      #   input del discrminator\n",
    "      d_input = torch.cat([genenerated_images, X.view(X.size(0), -1)])\n",
    "      #   gorund truth para el discriminator\n",
    "      d_gt = torch.cat([torch.zeros(X.size(0)), torch.ones(X.size(0))]).view(-1,1).to(device)\n",
    "      #   optimización\n",
    "      d_optimizer.zero_grad()\n",
    "      d_output = d(d_input)\n",
    "      d_l = crit(d_output, d_gt)\n",
    "      d_l.backward()\n",
    "      d_optimizer.step()\n",
    "      d_loss.append(d_l.item())\n",
    "      # entrenamos el generador\n",
    "      g.train()\n",
    "      d.eval()\n",
    "      #   generamos un batch de imágenes falsas\n",
    "      noise = torch.randn((X.size(0), g.input_size)).to(device)\n",
    "      genenerated_images = g(noise)\n",
    "      #   salidas del discriminador\n",
    "      d_output = d(genenerated_images)\n",
    "      #   gorund truth para el generator\n",
    "      g_gt = torch.ones(X.size(0)).view(-1,1).to(device)\n",
    "      #   optimización\n",
    "      g_optimizer.zero_grad()\n",
    "      g_l = crit(d_output, g_gt)\n",
    "      g_l.backward()\n",
    "      g_optimizer.step()\n",
    "      g_loss.append(g_l.item())\n",
    "      # logs\n",
    "      mb.child.comment = f'g_loss {np.mean(g_loss):.5f} d_loss {np.mean(d_loss):.5f}'\n",
    "    mb.write(f'Epoch {epoch}/{epochs} g_loss {np.mean(g_loss):.5f} d_loss {np.mean(d_loss):.5f}')\n",
    "    hist['g_loss'].append(np.mean(g_loss))    \n",
    "    hist['d_loss'].append(np.mean(d_loss))\n",
    "  return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejecutamos el entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "hist = fit(generator, discriminator, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualización de las pérdidas en el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame(hist)\n",
    "df.plot(grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probando generador entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "  noise = torch.randn((10, generator.input_size)).to(device)\n",
    "  generated_images = generator(noise)\n",
    "  fig, axs = plt.subplots(2,5,figsize=(15,5))\n",
    "  i = 0\n",
    "  for ax in axs:\n",
    "    for _ax in ax:\n",
    "      img = generated_images[i].view(28,28).cpu()\n",
    "      _ax.imshow(img, cmap='gray')\n",
    "      i+=1\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DCGANs\n",
    "\n",
    "Podemos usar **DCGANs** (_deep convolutional GANs_) para obtener mejores generadores utilizando redes convolucionales. En este caso, necesitaremos arquitecturas diferentes para generador y discriminador.\n",
    "\n",
    "<center>\n",
    "    <img src='images/dcgan.png'style=\"width: 900px;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arquitecura\n",
    "\n",
    "El **generador** recibirá un vector de 100 valores aleatorios y después aplicaremos varias capas de convoluciones transpuestas (que aumentarán el tamaño de los mapas de caracterísitcas, como vimos en las redes para segmentación) hasta obtener la imagen generada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.input_size = 100\n",
    "    self.inp = nn.Sequential(\n",
    "        nn.Linear(self.input_size, 7*7*128),\n",
    "        nn.BatchNorm1d(7*7*128),\n",
    "    )\n",
    "    self.main = nn.Sequential(\n",
    "        nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1, bias=False),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.inp(x)\n",
    "    x = x.view(-1, 128, 7, 7)\n",
    "    x = self.main(x)\n",
    "    x = x.view(x.size(0), 28*28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Configurando el generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "output = generator(torch.randn(64, 100))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Al final del generador usamos una activación **tanh**, que dará valores entre -1 y 1. Por este motivo tenemos que **re-normalizar nuestras imágenes en el dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, trainset):\n",
    "    self.imgs = torch.tensor([np.array(i[0]).flatten() / 255. for i in trainset], dtype=torch.float, device=device)\n",
    "    self.imgs = self.imgs * 2. - 1.\n",
    "    self.labels = torch.tensor([i[1] for i in trainset], dtype=torch.long, device=device)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.imgs)\n",
    "\n",
    "  def __getitem__(self, ix):\n",
    "    return self.imgs[ix], self.labels[ix]\n",
    "\n",
    "train = Dataset(trainset)\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "img, label = train[0]\n",
    "img.shape, img.dtype, img.max(), img.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\n",
    "\n",
    "imgs, labels = next(iter(dataloader))\n",
    "imgs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discriminador\n",
    "\n",
    "Usaremos CNN típica como las que conocemos cuando trabajamos en clasificación de imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.main = nn.Sequential(\n",
    "        nn.Conv2d(1, 64, 4, stride=2, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(True)\n",
    "    )\n",
    "    self.out = nn.Sequential(\n",
    "        nn.Linear(128*7*7, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    # esperamos vectores a la entrada de 28*28\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    x = self.main(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = self.out(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "output = discriminator(torch.randn(64, 28*28))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entrenamos la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "hist = fit(generator, discriminator, dataloader, crit=torch.nn.BCELoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizamos la pérdida del modelo para el generador y el discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(hist)\n",
    "df.plot(grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Revisamos los modelos generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "  noise = torch.randn((10, generator.input_size)).to(device)\n",
    "  generated_images = generator(noise)\n",
    "  fig, axs = plt.subplots(2,5,figsize=(15,5))\n",
    "  i = 0\n",
    "  for ax in axs:\n",
    "    for _ax in ax:\n",
    "      img = generated_images[i].view(28,28).cpu()\n",
    "      _ax.imshow(img, cmap='gray')\n",
    "      i+=1\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ejemplo real\n",
    "\n",
    "* Generador de caras: https://thispersondoesnotexist.com/\n",
    "* Generador de gatos: https://thiscatdoesnotexist.com/\n",
    "* Generador de arte: https://thisartworkdoesnotexist.com\n",
    "\n",
    "<center>\n",
    "    <img src='images/fake-face.jpeg'style=\"width: 500px;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /home/jbekios/anaconda3/envs/jupyter-for-class/lib/python3.7/site-packages (3.12.2)\n",
      "Requirement already satisfied: filelock in /home/jbekios/anaconda3/envs/jupyter-for-class/lib/python3.7/site-packages (from gdown) (3.0.12)\n",
      "Requirement already satisfied: six in /home/jbekios/anaconda3/envs/jupyter-for-class/lib/python3.7/site-packages (from gdown) (1.15.0)\n",
      "Requirement already satisfied: tqdm in /home/jbekios/anaconda3/envs/jupyter-for-class/lib/python3.7/site-packages (from gdown) (4.50.2)\n",
      "Requirement already satisfied: requests[socks] in /home/jbekios/anaconda3/envs/jupyter-for-class/lib/python3.7/site-packages (from gdown) (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jbekios/anaconda3/envs/jupyter-for-class/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jbekios/anaconda3/envs/jupyter-for-class/lib/python3.7/site-packages (from requests[socks]->gdown) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/jbekios/anaconda3/envs/jupyter-for-class/lib/python3.7/site-packages (from requests[socks]->gdown) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/jbekios/anaconda3/envs/jupyter-for-class/lib/python3.7/site-packages (from requests[socks]->gdown) (4.0.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /home/jbekios/anaconda3/envs/jupyter-for-class/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: natsort in /home/jbekios/anaconda3/envs/jupyter-for-class/lib/python3.7/site-packages (7.1.1)\n",
      "Requirement already satisfied: pillow<7.0.0 in /home/jbekios/anaconda3/envs/jupyter-for-class/lib/python3.7/site-packages (6.2.2)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'transform' from 'torchvision' (/home/jbekios/anaconda3/envs/jupyter-for-class/lib/python3.7/site-packages/torchvision/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f06cd53011b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m## Setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'transform' from 'torchvision' (/home/jbekios/anaconda3/envs/jupyter-for-class/lib/python3.7/site-packages/torchvision/__init__.py)"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "!pip install natsort\n",
    "!pip install 'pillow<7.0.0'\n",
    "\n",
    "\n",
    "import os\n",
    "import zipfile \n",
    "import gdown\n",
    "import torch\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transform\n",
    "\n",
    "## Setup\n",
    "# Number of gpus available\n",
    "ngpu = 1\n",
    "device = torch.device('cuda:0' if (\n",
    "    torch.cuda.is_available() and ngpu > 0) else 'cpu')\n",
    "\n",
    "## Fetch data from Google Drive \n",
    "# Root directory for the dataset\n",
    "data_root = 'data/celeba'\n",
    "# Path to folder with the dataset\n",
    "dataset_folder = f'{data_root}/img_align_celeba'\n",
    "# URL for the CelebA dataset\n",
    "url = 'https://drive.google.com/uc?id=1cNIac61PSA_LqDFYFUeyaQYekYPc75NH'\n",
    "# Path to download the dataset to\n",
    "download_path = f'{data_root}/img_align_celeba.zip'\n",
    "\n",
    "# Create required directories \n",
    "if not os.path.exists(data_root):\n",
    "  os.makedirs(data_root)\n",
    "  os.makedirs(dataset_folder)\n",
    "\n",
    "# Download the dataset from google drive\n",
    "gdown.download(url, download_path, quiet=False)\n",
    "\n",
    "# Unzip the downloaded file \n",
    "with zipfile.ZipFile(download_path, 'r') as ziphandler:\n",
    "  ziphandler.extractall(dataset_folder)\n",
    "\n",
    "## Create a custom Dataset class\n",
    "class CelebADataset(Dataset):\n",
    "  def __init__(self, root_dir, transform=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      root_dir (string): Directory with all the images\n",
    "      transform (callable, optional): transform to be applied to each image sample\n",
    "    \"\"\"\n",
    "    # Read names of images in the root directory\n",
    "    image_names = os.listdir(root_dir)\n",
    "\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform \n",
    "    self.image_names = natsorted(image_names)\n",
    "\n",
    "  def __len__(self): \n",
    "    return len(self.image_names)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # Get the path to the image \n",
    "    img_path = os.path.join(self.root_dir, self.image_names[idx])\n",
    "    # Load image and convert it to RGB\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    # Apply transformations to the image\n",
    "    if self.transform:\n",
    "      img = self.transform(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "## Load the dataset \n",
    "# Path to directory with all the images\n",
    "img_folder = f'{dataset_folder}/img_align_celeba'\n",
    "# Spatial size of training images, images are resized to this size.\n",
    "image_size = 64\n",
    "# Transformations to be applied to each individual image sample\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                          std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "# Load the dataset from file and apply transformations\n",
    "celeba_dataset = CelebADataset(img_folder, transform)\n",
    "\n",
    "## Create a dataloader \n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "# Number of workers for the dataloader\n",
    "num_workers = 0 if device.type == 'cuda' else 2\n",
    "# Whether to put fetched data tensors to pinned memory\n",
    "pin_memory = True if device.type == 'cuda' else False\n",
    "\n",
    "celeba_dataloader = torch.utils.data.DataLoader(celeba_dataset,\n",
    "                                                batch_size=batch_size,\n",
    "                                                num_workers=num_workers,\n",
    "                                                pin_memory=pin_memory,\n",
    "                                                shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
